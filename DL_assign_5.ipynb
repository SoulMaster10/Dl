{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb65a01-435d-4225-ac37-8cdeff8ecf02",
   "metadata": {},
   "source": [
    "5. Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "<br>\n",
    "a. Data preparation\n",
    "<br>\n",
    "b. Generate training data\n",
    "<br>\n",
    "c. Train model\n",
    "<br>\n",
    "d. Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa89a6-398f-49c0-82bf-f44aceb39f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ankita nandedkar/AppData/Local/Programs/Python/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,\\\n",
    "\tEmbedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c021937-7e54-4167-8efd-841824fbafcc",
   "metadata": {},
   "source": [
    "#### a. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3a2ac-5050-4889-b3d9-74400034ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8f9c-3266-4f07-b7e6-b56b628dd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for importing data from txt file\n",
    "# with open(\"data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9feb2-baec-4b6f-b697-12ce66881706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bf257-d6c8-4b73-9b23-73aedeeea4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a58d6-8e83-443f-9038-76afad3003de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data\n",
    "clean_sentences = []\n",
    "for sentence in sentences:\n",
    "    # skip empty string\n",
    "    if sentence == \"\":\n",
    "        continue;\n",
    "    # remove special characters\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "    # remove 1 letter words\n",
    "    sentence = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentence).strip()\n",
    "    # lower all characters\n",
    "    sentence = sentence.lower()\n",
    "    clean_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286c8b6-ae81-4c6c-926e-31fe6fc3ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c034d4e-321d-4416-ad5e-b9d3d6fb6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus\n",
    "corpus = clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218e589-294c-4892-8e4c-6a0b46d63cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corpus to a sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"After converting our words in the corpus \\\n",
    "into vector of integers:\")\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be572280-b97d-4175-9da2-035b134b311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for word to index and index to word\n",
    "index_to_word_map = {}\n",
    "word_to_index_map = {}\n",
    "for index_1, sequence in enumerate(sequences):\n",
    "    print(sequence)\n",
    "    words_in_sentence = clean_sentences[index_1].split()\n",
    "    print(words_in_sentence)\n",
    "    for index_2, value in enumerate(sequence):\n",
    "        index_to_word_map[value] = words_in_sentence[index_2]\n",
    "        word_to_index_map[words_in_sentence[index_2]] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbee67-7458-4486-b589-284c7d453538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word_map)\n",
    "print(\"\\n\")\n",
    "print(word_to_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a247e23-8497-43a9-b415-971bd592090a",
   "metadata": {},
   "source": [
    "#### b. Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83dfb9-4a0d-4bdc-acec-4528ed388fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 10\n",
    "window_size = 2\n",
    "\n",
    "# Generate the context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in sequences:\n",
    "\tfor i in range(window_size, len(sequence) - window_size):\n",
    "\t\tcontext = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "\t\ttarget = sequence[i]\n",
    "\t\tcontexts.append(context)\n",
    "\t\ttargets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12175cc9-f261-4867-be56-fb0f5e652c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of training data\n",
    "for i in range(5):\n",
    "    words = []\n",
    "    target = index_to_word_map.get(targets[i])\n",
    "    for j in contexts[i]:\n",
    "        words.append(index_to_word_map.get(j))\n",
    "    print(words, \"=>\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec03a5-0b78-4b6e-8472-d777e3c12cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the contexts and targets to numpy arrays\n",
    "X = np.array(contexts)\n",
    "Y = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8590a96-3fd5-4460-b7bf-ead26c93f187",
   "metadata": {},
   "source": [
    "#### c. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e9e11-7802-43c3-93d0-c7d78a9e99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2 * window_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f936dd-06f2-4aea-a5f0-327fa3a7dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word embeddings\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the embeddings\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ab058-dc6c-4187-a904-f7331c376779",
   "metadata": {},
   "source": [
    "#### d. Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f908298-5229-4a70-8abe-895e27ae35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i, word in enumerate(tokenizer.word_index.keys()):\n",
    "    x, y = reduced_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5869e8c-a2b4-409b-a0d2-35971f63a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "test_sentenses = [\n",
    "    \"we are to study\",\n",
    "    \"create programs direct processes\",\n",
    "    \"spirits process study program\",\n",
    "    \"idea study people create\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0948436-5e59-49ae-8c9b-c65d5fd1f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_sentense in test_sentenses:\n",
    "    test_words = test_sentense.split(\" \")\n",
    "    print(\"Words: \", test_words)\n",
    "    x_test = []\n",
    "    for i in test_words:\n",
    "        x_test.append(word_to_index_map.get(i))\n",
    "    x_test = np.array([x_test])\n",
    "    print(\"Indexs: \", x_test)\n",
    "    test_predictions = model.predict(x_test)\n",
    "    y_pred = np.argmax(test_predictions[0])\n",
    "    print(\"Predictons: \",test_words, \" => \", index_to_word_map.get(y_pred))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
